{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Vocab size: 100002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3760566it [01:21, 46370.58it/s]\n",
      "417841it [00:07, 57656.94it/s]\n",
      "4000it [00:00, 58102.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time usage: 0:01:30\n",
      "<bound method Module.parameters of Model(\n",
      "  (embedding): Embedding(100002, 200)\n",
      "  (postion_embedding): Positional_Encoding(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (attention): Multi_Head_Attention(\n",
      "      (fc_Q): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (fc_K): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (fc_V): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (attention): Scaled_Dot_Product_Attention()\n",
      "      (fc): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (layer_norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (feed_forward): Position_wise_Feed_Forward(\n",
      "      (fc1): Linear(in_features=200, out_features=1024, bias=True)\n",
      "      (fc2): Linear(in_features=1024, out_features=200, bias=True)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (layer_norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (encoders): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (attention): Multi_Head_Attention(\n",
      "        (fc_Q): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (fc_K): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (fc_V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (attention): Scaled_Dot_Product_Attention()\n",
      "        (fc): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (layer_norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (feed_forward): Position_wise_Feed_Forward(\n",
      "        (fc1): Linear(in_features=200, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=200, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (layer_norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (attention): Multi_Head_Attention(\n",
      "        (fc_Q): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (fc_K): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (fc_V): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (attention): Scaled_Dot_Product_Attention()\n",
      "        (fc): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (layer_norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (feed_forward): Position_wise_Feed_Forward(\n",
      "        (fc1): Linear(in_features=200, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=200, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (layer_norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=12800, out_features=2, bias=True)\n",
      ")>\n",
      "Epoch [1/20]\n",
      "Iter:      0,  Train Loss:  0.74,  Train Acc: 49.22%,  Val Loss:   0.7,  Val Acc: 56.34%,  Time: 0:00:12 *\n",
      "Iter:    100,  Train Loss:  0.78,  Train Acc: 53.91%,  Val Loss:  0.75,  Val Acc: 56.74%,  Time: 0:00:25 \n",
      "Iter:    200,  Train Loss:  0.73,  Train Acc: 49.22%,  Val Loss:  0.73,  Val Acc: 56.78%,  Time: 0:00:38 \n",
      "Iter:    300,  Train Loss:  0.73,  Train Acc: 48.44%,  Val Loss:   0.7,  Val Acc: 56.98%,  Time: 0:00:51 \n",
      "Iter:    400,  Train Loss:  0.75,  Train Acc: 50.00%,  Val Loss:  0.73,  Val Acc: 56.89%,  Time: 0:01:04 \n",
      "Iter:    500,  Train Loss:  0.71,  Train Acc: 56.25%,  Val Loss:   0.7,  Val Acc: 57.17%,  Time: 0:01:18 \n",
      "Iter:    600,  Train Loss:  0.67,  Train Acc: 55.47%,  Val Loss:   0.7,  Val Acc: 57.34%,  Time: 0:01:31 \n",
      "Iter:    700,  Train Loss:  0.71,  Train Acc: 54.69%,  Val Loss:  0.68,  Val Acc: 57.95%,  Time: 0:01:44 *\n",
      "Iter:    800,  Train Loss:  0.72,  Train Acc: 55.47%,  Val Loss:  0.68,  Val Acc: 58.15%,  Time: 0:01:58 *\n",
      "Iter:    900,  Train Loss:  0.72,  Train Acc: 51.56%,  Val Loss:  0.67,  Val Acc: 58.46%,  Time: 0:02:12 *\n",
      "Iter:   1000,  Train Loss:   0.7,  Train Acc: 57.81%,  Val Loss:  0.69,  Val Acc: 57.59%,  Time: 0:02:25 \n",
      "Iter:   1100,  Train Loss:  0.71,  Train Acc: 56.25%,  Val Loss:  0.72,  Val Acc: 57.29%,  Time: 0:02:39 \n",
      "Iter:   1200,  Train Loss:  0.73,  Train Acc: 50.00%,  Val Loss:   0.7,  Val Acc: 57.57%,  Time: 0:02:52 \n",
      "Iter:   1300,  Train Loss:  0.67,  Train Acc: 60.16%,  Val Loss:  0.69,  Val Acc: 57.98%,  Time: 0:03:05 \n",
      "Iter:   1400,  Train Loss:  0.71,  Train Acc: 56.25%,  Val Loss:  0.68,  Val Acc: 58.27%,  Time: 0:03:19 \n",
      "Iter:   1500,  Train Loss:  0.68,  Train Acc: 58.59%,  Val Loss:   0.7,  Val Acc: 57.68%,  Time: 0:03:32 \n",
      "Iter:   1600,  Train Loss:  0.74,  Train Acc: 49.22%,  Val Loss:  0.68,  Val Acc: 58.34%,  Time: 0:03:45 \n",
      "Iter:   1700,  Train Loss:  0.67,  Train Acc: 60.16%,  Val Loss:  0.68,  Val Acc: 58.47%,  Time: 0:03:58 \n",
      "Iter:   1800,  Train Loss:  0.68,  Train Acc: 59.38%,  Val Loss:  0.67,  Val Acc: 58.70%,  Time: 0:04:11 \n",
      "Iter:   1900,  Train Loss:   0.7,  Train Acc: 57.03%,  Val Loss:  0.68,  Val Acc: 58.45%,  Time: 0:04:24 \n",
      "No optimization for a long time, auto-stopping...\n",
      "Test Loss:  0.69,  Test Acc: 53.15%\n",
      "Precision, Recall and F1-Score...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     支持biden     0.5181    0.9552    0.6718      2008\n",
      "     支持trump     0.6980    0.1044    0.1817      1992\n",
      "\n",
      "    accuracy                         0.5315      4000\n",
      "   macro avg     0.6080    0.5298    0.4267      4000\n",
      "weighted avg     0.6077    0.5315    0.4277      4000\n",
      "\n",
      "Confusion Matrix...\n",
      "[[1918   90]\n",
      " [1784  208]]\n",
      "Time usage: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "# coding: UTF-8\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from train_eval import train, init_network\n",
    "from importlib import import_module\n",
    "from utils import build_dataset, build_iterator, get_time_dif\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Chinese Text Classification')\n",
    "parser.add_argument('--model', type=str, required=True, help='choose a model: TextCNN, TextRNN,TextRCNN, TextRNN_Att, DPCNN, Transformer')\n",
    "parser.add_argument('--embedding', default='pre_trained', type=str, help='random or pre_trained')\n",
    "parser.add_argument('--word', default=False, type=bool, help='True for word, False for char')\n",
    "#args = parser.parse_args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = '/home/kayzhou/zhangyue/text'  # 数据集\n",
    "\n",
    "    # 推文利用glove初始化 embedding_tweet.npz, 随机初始化:random\n",
    "    embedding = \"embedding_tweet.npz\"\n",
    "    model_name = \"Transformer\"  # 'TextRCNN'  # TextCNN, TextRNN, FastText, TextRCNN, TextRNN_Att, DPCNN, Transformer\n",
    "    x = import_module(\"models.\" + model_name)\n",
    "    config = x.Config(dataset, embedding)\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    torch.cuda.manual_seed_all(1)\n",
    "    torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"Loading data...\")\n",
    "    vocab, train_data, dev_data, test_data = build_dataset(config,True)\n",
    "    train_iter = build_iterator(train_data, config)\n",
    "    dev_iter = build_iterator(dev_data, config)\n",
    "    test_iter = build_iterator(test_data, config)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "    # train\n",
    "    config.n_vocab = len(vocab)\n",
    "    model = x.Model(config).to(config.device)\n",
    "    if model_name != 'Transformer':\n",
    "        init_network(model)\n",
    "    print(model.parameters)\n",
    "    train(config, model, train_iter, dev_iter, test_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8d40850c2a0f24b66f067e47dfa0a8004f5b10da74c53ddf9ba21a3b5b0a20a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
