{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "executionInfo": {
     "elapsed": 516,
     "status": "error",
     "timestamp": 1653661629807,
     "user": {
      "displayName": "张悦",
      "userId": "05797549964769432427"
     },
     "user_tz": -480
    },
    "id": "Rx2wRULDyIO3",
    "outputId": "7fb23d89-5bee-4b6e-d312-5bdd4aae32fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'one_month_network_202010', 'epochs': 1000, 'hidden': 16, 'bases': -1, 'learnrate': 0.001, 'l2norm': 0.0005, 'pred_type': 'net'}\n",
      "898999 112376 112374\n",
      "y:1123749\n",
      "num_nodes:1435367\n",
      "support:1\n",
      "2\n",
      "Data loaded successfully!\n",
      "Epoch: 0000 pred_type:Graph train_loss= 623812.7500 train_acc= 0.3556 val_acc= 0.3567 test_acc= 0.3553 time= 6.5729\n",
      "Epoch: 0001 pred_type:Graph train_loss= 622496.3125 train_acc= 0.3878 val_acc= 0.3678 test_acc= 0.3663 time= 1.0836\n",
      "Epoch: 0002 pred_type:Graph train_loss= 621136.2500 train_acc= 0.9688 val_acc= 0.8641 test_acc= 0.8647 time= 0.8710\n",
      "Epoch: 0003 pred_type:Graph train_loss= 619723.0000 train_acc= 0.9688 val_acc= 0.8800 test_acc= 0.8821 time= 0.8442\n",
      "Epoch: 0004 pred_type:Graph train_loss= 618253.5000 train_acc= 0.9688 val_acc= 0.8804 test_acc= 0.8827 time= 0.8277\n",
      "Epoch: 0005 pred_type:Graph train_loss= 616724.8750 train_acc= 0.9688 val_acc= 0.8804 test_acc= 0.8827 time= 0.9160\n",
      "Epoch: 0006 pred_type:Graph train_loss= 615134.8750 train_acc= 0.9688 val_acc= 0.8804 test_acc= 0.8827 time= 0.7410\n",
      "Epoch: 0007 pred_type:Graph train_loss= 613481.1875 train_acc= 0.9688 val_acc= 0.8804 test_acc= 0.8827 time= 0.9802\n",
      "Epoch: 0008 pred_type:Graph train_loss= 611761.5625 train_acc= 0.9688 val_acc= 0.8804 test_acc= 0.8827 time= 0.8944\n",
      "Epoch: 0009 pred_type:Graph train_loss= 609973.8125 train_acc= 0.9688 val_acc= 0.8804 test_acc= 0.8827 time= 0.9092\n",
      "Epoch: 0010 pred_type:Graph train_loss= 608115.9375 train_acc= 0.9688 val_acc= 0.8804 test_acc= 0.8827 time= 0.8855\n",
      "Epoch: 0011 pred_type:Graph train_loss= 606185.5625 train_acc= 0.9688 val_acc= 0.8804 test_acc= 0.8827 time= 0.9555\n",
      "Epoch: 0012 pred_type:Graph train_loss= 604180.8125 train_acc= 0.9688 val_acc= 0.8804 test_acc= 0.8827 time= 0.9088\n",
      "Epoch: 0013 pred_type:Graph train_loss= 602099.3125 train_acc= 0.9688 val_acc= 0.8804 test_acc= 0.8826 time= 0.8798\n",
      "Epoch: 0014 pred_type:Graph train_loss= 599939.3750 train_acc= 0.9688 val_acc= 0.8796 test_acc= 0.8822 time= 0.8375\n",
      "Epoch: 0015 pred_type:Graph train_loss= 597698.6250 train_acc= 0.9688 val_acc= 0.8793 test_acc= 0.8816 time= 0.7679\n",
      "Too long time not up\n",
      "best_acc_result:\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"/home/kayzhou/zhangyue/text/text_UNION/text_GCN\")\n",
    "\n",
    "from model import GraphConvolution, GCNModel\n",
    "from utils import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "\n",
    "        print(args)\n",
    "        \n",
    "        # Define Parameters\n",
    "        self.DATASET = args['dataset']\n",
    "        self.NB_EPOCH = args['epochs']\n",
    "        self.LR = args['learnrate']\n",
    "        self.L2 = args['l2norm']\n",
    "        self.HIDDEN = args['hidden']\n",
    "        self.BASES = args['bases']\n",
    "        self.use_cuda = False   \n",
    "        self.PRED_TYPE = args['pred_type']   \n",
    "        self.USE_BIAS = False\n",
    "        self.MODEL_NAME = '%s_%s' % (self.DATASET,self.PRED_TYPE) \n",
    "        self.num_features = 16\n",
    "        self.log_step = 1\n",
    "        \n",
    "\n",
    "        # Load Data\n",
    "        #去掉文件名返回目录\n",
    "        dirname = \"/home/kayzhou/zhangyue/text/data_GCN/data_processed/\"\n",
    "        #utils load_data\n",
    "        raw_data, data = load_data(args, dirname, self.use_cuda)\n",
    "        A = raw_data['A']\n",
    "        y = raw_data['y']\n",
    "        #Series.to_dense()函数已返回给定系列对象的密集表示形式。\n",
    "        #它分配了内存以存储系列中的缺失值。当缺少大量数据时，密集表示对内存的效率不高。\n",
    "        y = np.array(y)\n",
    "        self.labels = y\n",
    "\n",
    "        self.idx_train = data['idx_train'] \n",
    "        self.idx_valid = data['idx_valid']  \n",
    "        self.idx_test= data['idx_test'] \n",
    "        self.idx_train_set = data['idx_train_set'] \n",
    "        self.idx_test_set = data['idx_test_set'] \n",
    "        self.inputs = data['inputs']\n",
    "        self.labels_train = data['labels_train'] \n",
    "        self.labels_valid = data['labels_valid']  \n",
    "        self.labels_test = data['labels_test']  \n",
    "        self.num_nodes = data['num_nodes']  \n",
    "        self.num_docs = data['num_docs']  \n",
    "        self.num_non_docs= data['num_non_docs']\n",
    "        self.node2adj = data['node2adj']\n",
    "        self.support = data['support']\n",
    "        self.idx_train_pre = (np.array(self.idx_train)+self.num_non_docs).tolist()\n",
    "        self.idx_valid_pre = (np.array(self.idx_valid)+self.num_non_docs).tolist()\n",
    "        self.idx_test_pre = (np.array(self.idx_test)+self.num_non_docs).tolist()\n",
    "\n",
    "\n",
    "        print('Data loaded successfully!')\n",
    "\n",
    "        # Compile Model\n",
    "        #定义模型的基本参数\n",
    "        self.model = GCNModel(data, self.num_features, self.num_nodes, self.HIDDEN, self.support, self.BASES, 2, \n",
    "             bias_feature=self.USE_BIAS)\n",
    "        if (self.use_cuda):\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "        parameters = [p for p in list(self.model.parameters())\n",
    "            if p.requires_grad]\n",
    "        # for p in parameters:\n",
    "        #     print(p.size())\n",
    "        self.optimizer = optim.Adam(parameters,\n",
    "                            lr=self.LR, weight_decay=self.L2)\n",
    "\n",
    "        #lstm为sum\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    \n",
    "    def _train(self):\n",
    "        # Fit Model\n",
    "        best_acc, best_test_acc = 0, 0\n",
    "        best_loss = 10000000\n",
    "        best_train_loss = 1000000\n",
    "        best_epoch = 0\n",
    "        loss_list = []\n",
    "        train_acc_list = []\n",
    "        test_acc_list = []\n",
    "        dev_acc_list = []\n",
    "        for epoch in range(0, self.NB_EPOCH):\n",
    "            # Log wall-clock time\n",
    "            t = time.time()\n",
    "\n",
    "            self.model.zero_grad()\n",
    "\n",
    "            # Single training iteration\n",
    "            embeds_0 = self.inputs[0] \n",
    "            embeds_1 = self.model.gc1([embeds_0] + self.inputs[1:])\n",
    "            embeds_2 = self.model.gc2([embeds_1] + self.inputs[1:])\n",
    "            embeds_final = embeds_2\n",
    "            scores = self.model.clf_bias(embeds_2)\n",
    "            \n",
    "            \n",
    "            #只用文本节点预测的loss\n",
    "            loss_train = self.cross_entropy_loss(scores[self.idx_train_pre], self.labels_train) \n",
    "\n",
    "            preds = torch.argmax(scores, dim=1)\n",
    "            correct_train = torch.sum(preds[self.idx_train_pre] == self.labels_train)\n",
    "            correct_valid = torch.sum(preds[self.idx_valid_pre] == self.labels_valid)\n",
    "            correct_test = torch.sum(preds[self.idx_test_pre] == self.labels_test)\n",
    "            #计算准确率\n",
    "            train_acc = correct_train.item()/self.labels_train.size(0)\n",
    "            valid_acc = correct_valid.item()/self.labels_valid.size(0)\n",
    "            test_acc = correct_test.item()/self.labels_test.size(0)\n",
    "            \n",
    "            yuce_tag = preds[self.idx_test_pre] \n",
    "\n",
    "            print(\"Epoch: {:04d}\".format(epoch),\n",
    "                      \"pred_type:Graph\",\n",
    "                      \"train_loss= {:.4f}\".format(loss_train),\n",
    "                      \"train_acc= {:.4f}\".format(train_acc),\n",
    "                      \"val_acc= {:.4f}\".format(valid_acc),\n",
    "                      \"test_acc= {:.4f}\".format(test_acc),\n",
    "                      \"time= {:.4f}\".format(time.time() - t))  \n",
    "              \n",
    "            \n",
    "            if self.PRED_TYPE == \"shareu\" or self.PRED_TYPE == \"netshareu\":\n",
    "                scores_shareu = []\n",
    "                scores_value = scores.data.cpu().numpy()\n",
    "                for node in range(self.num_nodes):\n",
    "                    scores_t = np.zeros(2) \n",
    "\n",
    "                      #self.node2adj为doc节点 to share用户节点\n",
    "                    if (node not in self.node2adj):\n",
    "                        scores_shareu.append(scores_t)\n",
    "                        continue\n",
    "\n",
    "                    #分享文档的用户集合\n",
    "                    adj = list(self.node2adj[node])\n",
    "                    adj_coef = [1/len(adj)] * len(adj)\n",
    "\n",
    "                      #根据加权分享该文档的用户节点的预测得到文档预测\n",
    "                    for user, coef in zip(adj, adj_coef):\n",
    "                        scores_t += coef * scores_value[user]\n",
    "                    scores_shareu.append(scores_t) \n",
    "                  \n",
    "                scores_shareu = torch.FloatTensor(np.array(scores_shareu))\n",
    "              #scores_shareu = torch.FloatTensor(np.array(scores_shareu)).cuda()\n",
    "                loss_train = self.cross_entropy_loss(scores_shareu[self.idx_train_pre], self.labels_train) \n",
    "\n",
    "                preds_shareu = torch.argmax(scores_shareu, dim=1)\n",
    "                correct_train_shareu = torch.sum(preds_shareu[self.idx_train_pre] == self.labels_train)\n",
    "                correct_valid_shareu = torch.sum(preds_shareu[self.idx_valid_pre] == self.labels_valid)\n",
    "                correct_test_shareu = torch.sum(preds_shareu[self.idx_test_pre] == self.labels_test)\n",
    "                train_acc = correct_train_shareu.item()/self.labels_train.size(0)\n",
    "                valid_acc = correct_valid_shareu.item()/self.labels_valid.size(0)\n",
    "                test_acc = correct_test_shareu.item()/self.labels_test.size(0)\n",
    "                print(\"Epoch: {:04d}\".format(epoch),\n",
    "                    \"pred_type:User\",\n",
    "                      \"train_loss= {:.4f}\".format(loss_train),\n",
    "                      \"train_acc= {:.4f}\".format(train_acc),\n",
    "                      \"val_acc= {:.4f}\".format(valid_acc),\n",
    "                      \"test_acc= {:.4f}\".format(test_acc),\n",
    "                      \"time= {:.4f}\".format(time.time() - t))\n",
    "              \n",
    "                if self.PRED_TYPE == \"netshareu\":\n",
    "                    scores_shareu += scores\n",
    "                    loss_train = self.cross_entropy_loss(scores_shareu[self.idx_train_pre], self.labels_train) \n",
    "                    preds_netshareu = torch.argmax(scores_shareu, dim=1)\n",
    "\n",
    "                    correct_train_shareu = torch.sum(preds_netshareu[self.idx_train_pre] == self.labels_train)\n",
    "                    correct_valid_shareu = torch.sum(preds_netshareu[self.idx_valid_pre] == self.labels_valid)\n",
    "                    correct_test = torch.sum(preds_netshareu[self.idx_test_pre] == self.labels_test)\n",
    "                    train_acc = correct_train_shareu.item()/self.labels_train.size(0)\n",
    "                    valid_acc = correct_valid_shareu.item()/self.labels_valid.size(0)\n",
    "                    test_acc = correct_test_shareu.item()/self.labels_test.size(0)\n",
    "                    print(\"Epoch: {:04d}\".format(epoch),\n",
    "                          \"pred_type:Graph+User\",\n",
    "                            \"train_loss= {:.4f}\".format(loss_train),\n",
    "                            \"train_acc= {:.4f}\".format(train_acc),\n",
    "                            \"val_acc= {:.4f}\".format(valid_acc),\n",
    "                            \"test_acc= {:.4f}\".format(test_acc),\n",
    "                            \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "\n",
    "            loss_train.requires_grad_(True)\n",
    "            loss_train.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            loss_list.append(loss_train)\n",
    "            train_acc_list.append(train_acc)\n",
    "            dev_acc_list.append(valid_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "\n",
    "            #最好的结果仍旧是准确率最高的结果\n",
    "            if (test_acc  > best_acc):\n",
    "                best_acc = test_acc \n",
    "                best_acc_epoch = epoch\n",
    "                best_epoch = epoch\n",
    "                best_yuce_tag = yuce_tag\n",
    "                torch.save(self.model.state_dict(), '/home/kayzhou/zhangyue/text/text_UNION/text_GCN/saved_models/%s' % self.MODEL_NAME)\n",
    "            \n",
    "            \n",
    "            #用于判断是否结束训练\n",
    "            # if loss_train < best_train_loss:\n",
    "            #     best_train_loss = loss_train\n",
    "            #     best_epoch = epoch\n",
    "\n",
    "            if epoch-best_epoch > 10:\n",
    "                print(\"Too long time not up\")\n",
    "                break\n",
    "        \n",
    "        # plt.figure(figsize=(20,8),dpi=200)\n",
    "        # plt.plot(range(1,len(loss_list)+1),loss_list,\n",
    "        #   marker = \"o\") \n",
    "        # plt.xlabel('epoch')\n",
    "        # plt.ylabel('GCN_loss')\n",
    "        # plt.savefig(\"/content/drive/MyDrive/exp_GCN/图神经网络/tupian.png\",dpi=200)\n",
    "        # plt.show()\n",
    "        \n",
    "        # plt.figure(figsize=(20,8),dpi=200)\n",
    "        # plt.plot(range(1,len(loss_list)+1),train_acc_list,marker = \"o\",label = \"train accuracy\") \n",
    "        # plt.plot(range(1,len(loss_list)+1),dev_acc_list,marker = \"o\",label = \"dev accuracy\")\n",
    "        # plt.plot(range(1,len(loss_list)+1),test_acc_list,marker = \"o\",label = \"test accuracy\")\n",
    "        # plt.xlabel('epoch')\n",
    "        # plt.ylabel('accuracy')\n",
    "        # plt.savefig(\"/content/drive/MyDrive/exp_GCN/图神经网络/acc.png\",dpi=200)\n",
    "        # plt.show()\n",
    "    \n",
    "\n",
    "        print(\"best_acc_result:\")\n",
    "        print(best_acc_epoch)\n",
    "\n",
    "    \n",
    "            \n",
    "\n",
    "        #print(best_result)\n",
    "        # fout = open('/content/drive/MyDrive/exp_GCN/logs/result_%s.txt' % self.DATASET, 'a')\n",
    "        # fout.write(str(best_result) + '\\n')\n",
    "        # fout.close()\n",
    "        return self.labels_test,best_yuce_tag,loss_list,train_acc_list,dev_acc_list,test_acc_list\n",
    " \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hyper Parameters\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"-d\", \"--dataset\", type=str, default=\"one_month_network_202010\",\n",
    "                    help=\"Dataset string\")\n",
    "    ap.add_argument(\"-e\", \"--epochs\", type=int, default=1000,\n",
    "                    help=\"Number training epochs\")\n",
    "    ap.add_argument(\"-hd\", \"--hidden\", type=int, default=16,\n",
    "                    help=\"Number hidden units\")\n",
    "    ap.add_argument(\"-b\", \"--bases\", type=int, default=-1,\n",
    "                    help=\"Number of bases used (-1: all)\")\n",
    "    ap.add_argument(\"-lr\", \"--learnrate\", type=float, default=0.001,\n",
    "                    help=\"Learning rate\")\n",
    "    ap.add_argument(\"-l2\", \"--l2norm\", type=float, default=5e-4,\n",
    "                    help=\"L2 normalization of input weights\")\n",
    "    ap.add_argument(\"-pt\", \"--pred_type\", type=str, default=\"net\",\n",
    "                    help=\"prediction method: select from 'net'/'shareu'/'netshareu'\")\n",
    "    args = vars(ap.parse_args(args=[]))\n",
    "    dataset = args['dataset']\n",
    "\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    torch.cuda.manual_seed(1)\n",
    "    \n",
    "\n",
    "    trainer = Trainer(args)\n",
    "    true_labels_test,best_yuce_tag,loss_list,train_acc_list,dev_acc_list,test_acc_list = trainer._train()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8591    0.9785    0.9149     72448\n",
      "           1     0.9478    0.7089    0.8111     39926\n",
      "\n",
      "    accuracy                         0.8827    112374\n",
      "   macro avg     0.9035    0.8437    0.8630    112374\n",
      "weighted avg     0.8906    0.8827    0.8780    112374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_labels_test,best_yuce_tag,digits=4))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNvOqa9Li3Q9Oyg70CzbUrd",
   "collapsed_sections": [],
   "mount_file_id": "1tyXVaE6kz3XeyCRpIIlgUSlv3d_ZhgLw",
   "name": "run.ipynb（副本）",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e8d40850c2a0f24b66f067e47dfa0a8004f5b10da74c53ddf9ba21a3b5b0a20a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
